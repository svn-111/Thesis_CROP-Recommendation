{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zlC0D-NcerP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import  precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Defence/defence - Sheet1.csv')\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.get_dummies(data[['Land_type','Land_level','Water_recession','Drainage','Soil_texture','Consistency','Salinity','Available_soil_moisture','Periods','ph','Condition','Production(t/ha)','N','P','K','Temperature','Rainfall(mm)']])\n",
        "\n",
        "y = data['Label']"
      ],
      "metadata": {
        "id": "M3qwIc-icgW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "g9v-mUt_ckx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_classifier = SVC(kernel='linear')\n"
      ],
      "metadata": {
        "id": "fMs-RkEGcnsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "WPYyhvXLcnve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svm_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "MnTO-Oaacnzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model# Calculate the accuracy of the model\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "id": "mxwBXW4zcn2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "3_ghoD4Tcn5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate confusion matrix for accuracy\n",
        "conf_matrix_accuracy = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix for accuracy\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_accuracy, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix for Accuracy')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate confusion matrix for precision\n",
        "conf_matrix_precision = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Plot confusion matrix for precision\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_precision, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix for Precision')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Generate confusion matrix for recall\n",
        "conf_matrix_recall = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Plot confusion matrix for recall\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_recall, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix for Recall')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# Generate confusion matrix for F1-score\n",
        "conf_matrix_f1 = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Plot confusion matrix for F1-score\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_f1, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix for F1-score')\n",
        "plt.show()\n",
        "\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Calculate additional metrics\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "TbLlt7EScn83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}